# Local Models Service Configuration
# Copy this file to .env and update with your settings

# API/Service Configuration
SERVICE_PORT=8000
LOG_LEVEL=INFO

# Model Selection
# For text generation: mistralai/Mistral-7B-Instruct-v0.1, meta-llama/Llama-2-7b-chat-hf, etc.
LLAMA_MODEL_NAME=mistralai/Mistral-7B-Instruct-v0.1

# For image generation: stabilityai/stable-diffusion-xl-base-1.0
SDXL_MODEL_NAME=stabilityai/stable-diffusion-xl-base-1.0

# Optional: SDXL refiner for better quality (not used in basic pipeline)
SDXL_REFINER_NAME=stabilityai/stable-diffusion-xl-refiner-1.0

# Model cache directory (where HuggingFace models are downloaded and cached)
MODELS_DIR=./models

# Inference Parameters
NUM_INFERENCE_STEPS=30
GUIDANCE_SCALE=7.5

# Device selection (cuda or cpu)
# DEVICE=cuda  # uncomment if you want to force CUDA
